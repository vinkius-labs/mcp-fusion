# Cost Reduction & Anti-Hallucination

## Before & After

### Before: How MCP Servers Are Built Today

**Step 1 â€” Every MCP server is a monolithic switch/case.**

Open any MCP server on GitHub. You'll find the same architecture: one handler function, one `switch` statement, and `JSON.stringify()` as the entire response strategy. No validation. No separation of concerns. No perception layer. As the number of operations grows, the handler becomes a monolith:

```typescript
// This is the reality of MCP servers today.
// Every server in the ecosystem follows this pattern.
server.setRequestHandler(CallToolRequestSchema, async (request) => {
    const { name, arguments: args } = request.params;

    switch (name) {
        case 'create_user':
            const user = await db.users.create(args);  // no validation
            return { content: [{ type: 'text', text: JSON.stringify(user) }] };
            // â†‘ leaks tenant_id, password_hash, internal_flags to the LLM

        case 'get_user':
            const found = await db.users.findUnique({ where: { id: args.id } });
            return { content: [{ type: 'text', text: JSON.stringify(found) }] };

        case 'update_user':
            // copy-paste from create_user with minor changes
            const updated = await db.users.update({ where: { id: args.id }, data: args });
            return { content: [{ type: 'text', text: JSON.stringify(updated) }] };

        case 'list_invoices':
            const invoices = await db.invoices.findMany();  // no limit, returns 10,000 rows
            return { content: [{ type: 'text', text: JSON.stringify(invoices) }] };
            // â†‘ 10,000 rows Ã— ~500 tokens = 5,000,000 tokens in one response

        // ... 46 more cases, same pattern, growing into a 2,000-line file
    }
});
```

No input validation â€” the LLM can send anything. No output filtering â€” internal fields leak. No domain context â€” the agent gets raw data and guesses. No guardrails â€” a single `findMany()` can blow through the context window. And as the server grows from 5 tools to 50, the switch/case becomes an unmaintainable monolith.

**Step 2 â€” The company compensates with a system prompt.**

Since the tools can't teach the LLM anything, the company writes a **book of instructions** in the system prompt â€” rules for every domain entity, every edge case, every formatting convention:

```text
System Prompt (sent on EVERY LLM call, regardless of what tool is being used):

"When displaying invoices, amount_cents is in cents. Always divide by 100..."
"For users, mask email addresses for non-admin roles..."
"Task statuses use emojis: ğŸ”„ In Progress, âœ… Done, âŒ Blocked..."
"Sprint velocity is calculated as completed story points / sprint days..."
"Project budgets are always in USD. Format as $XX,XXX.00..."
"When showing reports, always include the date range in the header..."
"Never display fields: tenant_id, password_hash, internal_flags..."
... (50+ rules for 15+ domain entities)

~2,000 tokens. Sent even when the agent is just calling tasks.list
and needs none of these invoice, sprint, or budget rules.
```

The company is sending a book to an endpoint that doesn't need it. Every single LLM call â€” even a simple `tasks.list` â€” pays the full price for invoice formatting rules, sprint velocity formulas, and budget conventions it will never use.

**Step 3 â€” Every operation is a separate tool.**

50 operations = 50 tool definitions, each with name, description, and JSON schema. All 50 are injected into the LLM's context on every conversation turn:

```text
Tool 1/50: create_user        â€” ~180 tokens (name + description + inputSchema)
Tool 2/50: get_user            â€” ~160 tokens
Tool 3/50: update_user         â€” ~210 tokens
...
Tool 50/50: export_report      â€” ~190 tokens

Total: ~10,000 tokens of tool schemas, on every turn.
```

The agent needs 1-2 tools for the current task. It pays for 50.

**The result:**

```text
~10,000 tokens (50 tool schemas)
+ ~2,000 tokens (system prompt book)
= ~12,000 tokens of prompt tax per turn â€” mostly irrelevant noise.

The agent picks the wrong tool â†’ retry (re-pays 12,000 tokens).
The agent invents a parameter â†’ retry (re-pays 12,000 tokens).
The agent guesses wrong about the data â†’ user corrects â†’ re-pays again.
```

### After: mcp-fusion with MVA

Same 50 operations. The LLM calls `tools/list`:

```text
Tool 1/5: users     â€” 350 tokens (6 actions: list, get, create, update, delete, invite)
Tool 2/5: projects  â€” 340 tokens (5 actions: list, get, create, update, archive)
Tool 3/5: billing   â€” 380 tokens (8 actions: list, get, create, pay, refund, ...)
Tool 4/5: tasks     â€” 320 tokens (6 actions: list, get, create, update, assign, close)
Tool 5/5: reports   â€” 280 tokens (3 actions: generate, export, schedule)

Total: ~1,670 tokens. Same 50 operations.
System prompt domain rules: 0 tokens. Rules travel with data (see below).
```

From ~12,000 tokens to ~1,670. No book of instructions in the system prompt â€” domain rules are injected **just-in-time** only when the agent receives data from that domain.

Then the tool responds â€” not with raw JSON, but with a **structured perception package**:

```text
Content Block 1 â€” DATA (Zod-validated, only declared fields):
{"id":"INV-001","amount_cents":45000,"status":"pending"}

Content Block 2 â€” SERVER-RENDERED UI:
[echarts gauge chart config]
[SYSTEM]: Pass this echarts block directly to the user interface.

Content Block 3 â€” DOMAIN RULES (JIT, scoped to this domain only):
[DOMAIN RULES]:
- CRITICAL: amount_cents is in CENTS. Divide by 100 before display.
- Use currency format: $XX,XXX.00
- Use status emojis: âœ… paid, â³ pending, ğŸ”´ overdue

Content Block 4 â€” NEXT ACTIONS (computed from data state):
[SYSTEM HINT]: Based on the current state, recommended next tools:
  â†’ billing.pay: Process immediate payment
  â†’ billing.send_reminder: Send payment reminder
```

No guessing. Undeclared fields rejected. Domain rules scoped. Next actions data-driven. Charts server-rendered.

**The agent gets it right the first time.** Fewer tokens in the prompt. Fewer retries. Faster response. Lower cost.

---

## The Design Thesis

<div class="equation-header">

> **The equation behind every design decision in mcp-fusion:**

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚    Fewer Tokens + Fewer Requests = Less Hallucination + Less Cost   â”‚
â”‚                                                                     â”‚
â”‚    â†“ Tokens per call             â†“ Retry loops                      â”‚
â”‚    â†“ Tools in context            â†“ Re-reads of stale data           â”‚
â”‚    â†“ Noise in responses          â†“ Correction calls                 â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚    = Faster responses Â· Lower API bills Â· Deterministic behavior    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

We believe cost and hallucination are not separate problems â€” they are **two symptoms of the same root cause**: too many tokens flowing through the LLM context window, and too many requests being made because the agent didn't get what it needed the first time.

Every design decision in mcp-fusion is guided by this principle. This page documents the mechanisms we've implemented so far to attack both sides of the equation.

---

## The Problem We're Solving

Every interaction with an LLM has a direct cost:

```text
Cost per call = (input_tokens + output_tokens) Ã— price_per_token
Total cost    = cost_per_call Ã— number_of_calls
```

But the **hidden cost** â€” the one that multiplies everything â€” comes from retry loops:

| Problem | What Tends to Happen | Cost Impact |
|---|---|---|
| **Context Saturation** | Too many tool schemas flood the prompt | Agent picks wrong tool â†’ retry |
| **Hallucinated Parameters** | Agent invents field names | Validation fails â†’ retry |
| **Ambiguous Data** | No domain rules â†’ agent guesses | Wrong output â†’ user corrects â†’ re-call |
| **Action Blindness** | Agent doesn't know next step | Hallucinates tool name â†’ error â†’ retry |
| **Stale Data** | Agent uses cached results after mutation | Wrong answer â†’ user notices â†’ re-call |
| **Context DDoS** | Thousands of rows returned unbounded | Massive token bill + context overflow |

Each retry is a full round-trip: input tokens + output tokens + latency + API cost. Our goal is to reduce these retries as close to zero as practical.

---

## Our Approach: 8 Mechanisms

We attack cost and hallucination through eight interconnected mechanisms. Each maps directly to code in the repository.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      The Anti-Hallucination Stack                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â‘  Action Consolidation        â†’ Fewer tools in context   â†’ â†“ tokens    â”‚
â”‚  â‘¡ TOON Encoding               â†’ Compact descriptions     â†’ â†“ tokens    â”‚
â”‚  â‘¢ Zod .strict()              â†’ No hallucinated params   â†’ â†“ retries   â”‚
â”‚  â‘£ Self-Healing Errors         â†’ Fix on first retry       â†’ â†“ retries   â”‚
â”‚  â‘¤ Cognitive Guardrails        â†’ Bounded response size    â†’ â†“ tokens    â”‚
â”‚  â‘¥ Agentic Affordances         â†’ Correct next action      â†’ â†“ retries   â”‚
â”‚  â‘¦ JIT Context (System Rules)  â†’ No guessing domain logic â†’ â†“ retries   â”‚
â”‚  â‘§ State Sync                  â†’ No stale-data re-reads   â†’ â†“ requests  â”‚
â”‚                                                                          â”‚
â”‚  Design goal: significant cost reduction + deterministic agent behavior  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â‘  Action Consolidation â€” Reducing Tool Count

**The problem:** Standard MCP servers create one tool per operation. 50 tools = 50 JSON schemas injected into the LLM's system prompt. The context window fills with schema metadata before the agent even sees the user's question.

**Our approach:**

Operations are grouped behind a single tool with a discriminator enum. The schema surface area shrinks significantly:

```typescript
// Instead of 6 individual tools (~1,200 tokens in the prompt),
// one grouped tool covers the same operations (~350 tokens)
const projects = defineTool<AppContext>('projects', {
    actions: {
        list:    { readOnly: true, handler: ... },
        get:     { readOnly: true, params: { id: 'string' }, handler: ... },
        create:  { params: { name: 'string' }, handler: ... },
        update:  { params: { id: 'string', name: 'string' }, handler: ... },
        archive: { destructive: true, params: { id: 'string' }, handler: ... },
        delete:  { destructive: true, params: { id: 'string' }, handler: ... },
    },
});
```

Under the hood, `SchemaGenerator.ts` compiles all actions into **one** `inputSchema` with a discriminator enum, and `applyAnnotations()` adds per-field context â€” telling the LLM which fields are needed for which action:

```typescript
// From: src/framework/schema/SchemaGenerator.ts
// Per-field annotations reduce parameter-guessing by providing explicit context
annotateField(properties, key, `Required for: ${tracking.requiredIn.join(', ')}`);
```

**What we're aiming for:**

| Metric | Without Consolidation | With Consolidation |
|---|---|---|
| Tools in prompt | 50 | 1-5 |
| Approximate schema tokens | ~10,000 | ~1,500 |
| Tool-selection ambiguity | Higher | Reduced |

---

## â‘¡ TOON Encoding â€” Compact Token Representation

**The problem:** Tool descriptions and responses use verbose JSON, spending tokens on structural characters (`{`, `}`, `"`, `:`) that carry no semantic information.

**Our approach:**

TOON (Token-Oriented Object Notation) replaces JSON structure with compact pipe-delimited tabular data â€” both in tool descriptions and in response payloads:

```typescript
// From: src/framework/schema/ToonDescriptionGenerator.ts
function encodeFlatActions<TContext>(
    actions: readonly InternalAction<TContext>[],
): string {
    const rows = actions.map(a => buildActionRow(a.key, a));
    return encode(rows, { delimiter: '|' });
}
// Result: "action|desc|required\nlist|List projects|\nget|Get by ID|id"
```

For responses, `toonSuccess()` provides an opt-in encoding path:

```typescript
// From: src/framework/response.ts
export function toonSuccess(data: unknown, options?: EncodeOptions): ToolResponse {
    const defaults: EncodeOptions = { delimiter: '|' };
    const text = encode(data, { ...defaults, ...options });
    return { content: [{ type: "text", text }] };
}
```

Based on our testing, TOON achieves roughly **40-50% token reduction** over equivalent JSON for tabular data (source: `toonSuccess()` JSDoc). The savings compound across every call in a conversation.

---

## â‘¢ Zod `.strict()` â€” Preventing Parameter Hallucination

**The problem:** LLMs frequently invent parameter names. Without strict validation, these ghost fields can leak into handlers, causing silent bugs or unexpected behavior.

**Our approach:**

Every action's Zod schema is compiled with `.strict()` at build time. Undeclared fields are **explicitly rejected** with an actionable error telling the LLM exactly which fields are invalid:

```typescript
// From: src/framework/builder/ToolDefinitionCompiler.ts
function buildValidationSchema(action, commonSchema) {
    const base = applyCommonSchemaOmit(commonSchema, action.omitCommonFields);
    const specific = action.schema;
    const merged = base && specific ? base.merge(specific) : (base ?? specific);
    if (!merged) return null;
    return merged.strict();  // â† rejects all undeclared fields with actionable error
}
```

This validation happens in `ExecutionPipeline.ts` before the handler runs â€” making it physically impossible for hallucinated parameters to reach application code:

```typescript
// From: src/framework/execution/ExecutionPipeline.ts
const result = validationSchema.safeParse(argsWithoutDiscriminator);
// Valid: validated args go to handler
// Invalid: self-healing error (see mechanism â‘£)
```

---

## â‘£ Self-Healing Errors â€” Reducing Retry Loops

**The problem:** When validation fails, a generic error like `"Validation failed: email: Invalid"` gives the LLM no guidance on what format is expected. The agent tries blind variations â€” each costing a full round-trip.

**Our approach:**

`ValidationErrorFormatter.ts` translates Zod errors into directive correction prompts that aim to help the agent self-correct on the first retry:

```typescript
// From: src/framework/execution/ValidationErrorFormatter.ts
// Instead of: "Validation failed: email: Invalid"
// Produces actionable correction:
// "âŒ Validation failed for 'users.create':
//   â€¢ email â€” Invalid email format. You sent: 'admin@local'.
//     Expected: a valid email address (e.g. user@example.com).
//   â€¢ age â€” Number must be >= 18. You sent: 10.
//   ğŸ’¡ Fix the fields above and call the action again."
```

For business-logic errors, `toolError()` provides structured recovery guidance:

```typescript
// From: src/framework/response.ts
return toolError('ProjectNotFound', {
    message: `Project '${args.project_id}' does not exist.`,
    suggestion: 'Call projects.list first to get valid IDs, then retry.',
    availableActions: ['projects.list'],
});
```

The design goal is to bring the average retries-per-error as close to 1 as possible.

---

## â‘¤ Cognitive Guardrails â€” Bounding Response Size

**The problem:** A single `list_all` operation can return thousands of records. At ~500 tokens per record, that can mean millions of tokens in a single response â€” overwhelming the context window and generating significant API costs.

**Our approach:**

The Presenter's `.agentLimit()` truncates data **before** it reaches the LLM and injects a teaching block that guides the agent toward filters and pagination:

```typescript
// From: src/framework/presenter/Presenter.ts â€” make()
if (isArray && this._agentLimit && data.length > this._agentLimit.max) {
    const omitted = data.length - this._agentLimit.max;
    data = data.slice(0, this._agentLimit.max);
    truncationBlock = this._agentLimit.onTruncate(omitted);
}
```

Usage:

```typescript
const TaskPresenter = createPresenter('Task')
    .schema(taskSchema)
    .agentLimit(50, (omitted) =>
        ui.summary(`âš ï¸ Showing 50 of ${50 + omitted}. Use filters to narrow results.`)
    );
```

**Estimated cost impact (GPT-5.2, input @ $1.75/1M tokens):**

| Scenario | Rows | Tokens | Estimated Cost |
|---|---|---|---|
| No guardrail | 10,000 | ~5,000,000 | ~$8.75 |
| `.agentLimit(50)` | 50 | ~25,000 | ~$0.04 |

Beyond cost, the truncated response stays within the context window, which should help prevent the hallucination cascade that can occur when context overflows.

---

## â‘¥ Agentic Affordances â€” Guiding the Next Action

**The problem:** After receiving data, the agent must decide what to do next. Without guidance, it may hallucinate tool names or skip valid actions â€” each wrong decision is an avoidable API call.

**Our approach:**

`.suggestActions()` provides HATEOAS-style next-action hints based on data state, which we hope reduces wrong-tool selection:

```typescript
// From: src/framework/presenter/Presenter.ts
.suggestActions((invoice, ctx) => {
    if (invoice.status === 'pending') {
        return [
            { tool: 'billing.pay', reason: 'Process immediate payment' },
            { tool: 'billing.send_reminder', reason: 'Send payment reminder' },
        ];
    }
    return [];
})
```

The agent receives explicit context in the response:

```text
[SYSTEM HINT]: Based on the current state, recommended next tools:
  â†’ billing.pay: Process immediate payment
  â†’ billing.send_reminder: Send payment reminder
```

The principle is borrowed from REST's HATEOAS â€” the server tells the client what's possible, rather than leaving the client to guess.

---

## â‘¦ JIT Context â€” Domain Rules That Travel with Data

**The problem:** Global system prompts tend to grow into bloated documents with rules for every domain entity. The agent receives invoice rules when working with tasks. Context space is wasted, and misapplied rules can cause errors.

**Our approach:**

Rules travel **with the data**, not in the system prompt. We call this **Context Tree-Shaking** â€” domain rules only appear in the LLM's context when that specific domain is active:

```typescript
// From: src/framework/presenter/Presenter.ts â€” _attachRules()
if (typeof this._rules === 'function') {
    const resolved = this._rules(singleData, ctx)
        .filter((r): r is string => r !== null && r !== undefined);
    if (resolved.length > 0) builder.systemRules(resolved);
}
```

The agent sees rules **only** when they're relevant:

```text
[DOMAIN RULES]:
- CRITICAL: amount_cents is in CENTS. Always divide by 100 before display.
- Use currency format: $XX,XXX.00
- Use status emojis: âœ… paid, â³ pending, ğŸ”´ overdue
```

This should reduce both wasted tokens (irrelevant rules in the system prompt) and misapplication errors (applying the wrong domain's rules).

---

## â‘§ State Sync â€” Preventing Stale-Data Re-reads

**The problem:** After the agent calls `sprints.update`, its cached view of `sprints.list` is stale. Without a signal, the agent may use old data â€” producing incorrect answers. The user notices, asks again, and triggers an avoidable re-read.

**Our approach:**

State Sync injects causal invalidation signals at the protocol layer, inspired by [RFC 7234](https://tools.ietf.org/html/rfc7234) cache-control semantics:

```typescript
// From: src/framework/state-sync/CausalEngine.ts
// Safety: only invalidate on SUCCESS (failed mutation = state unchanged)
export function resolveInvalidations(policy, isError) {
    if (isError) return [];
    return policy?.invalidates ?? [];
}
```

After a successful mutation, the response includes a system block:

```text
[System: Cache invalidated for sprints.* â€” caused by sprints.update]
```

And tool descriptions carry cache-control directives:

```text
"Manage sprints. [Cache-Control: no-store]"
"List countries. [Cache-Control: immutable]"
```

Configuration:

```typescript
registry.attachToServer(server, {
    stateSync: {
        defaults: { cacheControl: 'no-store' },
        policies: [
            { match: 'sprints.update', invalidates: ['sprints.*'] },
            { match: 'tasks.update',   invalidates: ['tasks.*', 'sprints.*'] },
            { match: 'countries.*',    cacheControl: 'immutable' },
        ],
    },
});
```

---

## The Structured Perception Package â€” Exact Context for the LLM

Reducing tokens is only half of the equation. The other half is about **signal quality** â€” making sure every token that *does* reach the LLM carries maximum information density. We believe this is what makes the agent smarter: not just fewer tokens, but the *right* tokens at the *right* time.

mcp-fusion structures context at two layers. Everything described below is implemented in real code.

### Layer 1: Tool Definition (what the LLM sees in `tools/list`)

When the LLM starts a conversation, it receives the list of available tools. Each tool definition carries three types of precise context:

**1. Workflow Annotations in the Description**

`DescriptionGenerator.ts` generates a `Workflow:` section that tells the LLM exactly which parameters are required for each action and which actions are destructive:

```text
Manage projects. Actions: list, get, create, update, archive, delete

Workflow:
- 'get': Get project details. Requires: id
- 'create': Create new project. Requires: name
- 'update': Requires: id, name
- 'archive': Requires: id [DESTRUCTIVE]
- 'delete': Requires: id [DESTRUCTIVE]
```

The `[DESTRUCTIVE]` tag comes directly from the action's `destructive: true` flag in the builder. The LLM sees this before making any call.

**2. Per-Field Schema Annotations**

`SchemaGenerator.ts` adds precise per-field annotations to the JSON Schema, telling the LLM exactly which fields belong to which action:

```json
{
  "properties": {
    "action": { "type": "string", "enum": ["list", "get", "create", "update", "delete"] },
    "id":     { "type": "string", "description": "Required for: get, update, delete" },
    "name":   { "type": "string", "description": "Required for: create. For: update" },
    "status": { "type": "string", "description": "For: list" }
  }
}
```

This per-field context is generated by `applyAnnotations()` in `SchemaGenerator.ts`. A field that is required for some actions but optional for others gets a precise annotation like `"Required for: create. For: update"` â€” not a generic `"(optional)"`.

**3. Tool-Level Annotations**

`AnnotationAggregator.ts` aggregates per-action metadata into MCP standard annotations:

```json
{
  "readOnlyHint": false,
  "destructiveHint": true,
  "idempotentHint": false
}
```

These are resolved automatically: `readOnlyHint` is `true` only if **all** actions are read-only. `destructiveHint` is `true` if **any** action is destructive. The LLM receives behavioral metadata about the tool without having to infer it from descriptions.

### Layer 2: Tool Response (what the LLM sees in `tools/call`)

When a tool responds, the `ResponseBuilder.build()` method composes a multi-block MCP response. Each block is a separate `content` entry with a specific semantic purpose:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Structured Perception Package                         â”‚
â”‚              (exact output of ResponseBuilder.build())                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Block 1 â€” DATA                                                         â”‚
â”‚  Zod-validated, .strict()-ed JSON. Only declared fields.                â”‚
â”‚  {"id":"INV-001","amount_cents":45000,"status":"pending"}               â”‚
â”‚                                                                         â”‚
â”‚  Block 2 â€” UI BLOCKS (one content entry per block)                      â”‚
â”‚  Server-rendered charts/diagrams with pass-through instruction.         â”‚
â”‚  (echarts config as fenced code block)                                  â”‚
â”‚  [SYSTEM]: Pass this echarts block directly to the user interface.      â”‚
â”‚                                                                         â”‚
â”‚  Block 3 â€” EMBEDDED PRESENTER BLOCKS                                    â”‚
â”‚  Rules and UI blocks from child Presenters (via .embed()).              â”‚
â”‚  Merged automatically from ClientPresenter, ProductPresenter, etc.      â”‚
â”‚                                                                         â”‚
â”‚  Block 4 â€” LLM HINTS                                                    â”‚
â”‚  ğŸ’¡ This client has an overdue balance. Mention it proactively.         â”‚
â”‚                                                                         â”‚
â”‚  Block 5 â€” DOMAIN RULES                                                 â”‚
â”‚  [DOMAIN RULES]:                                                        â”‚
â”‚  - CRITICAL: amount_cents is in CENTS. Divide by 100 before display.   â”‚
â”‚  - Use currency format: $XX,XXX.00                                     â”‚
â”‚  - Use status emojis: âœ… paid, â³ pending, ğŸ”´ overdue                   â”‚
â”‚                                                                         â”‚
â”‚  Block 6 â€” ACTION SUGGESTIONS                                          â”‚
â”‚  [SYSTEM HINT]: Based on the current state, recommended next tools:     â”‚
â”‚    â†’ billing.pay: Process immediate payment                             â”‚
â”‚    â†’ billing.send_reminder: Send payment reminder                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Every block above is generated from real code in `ResponseBuilder.ts` (lines 239-281). The block order, the prefix markers (`[DOMAIN RULES]`, `[SYSTEM HINT]`, `ğŸ’¡`, `[SYSTEM]`), and the formatting are all deterministic â€” they come directly from the builder, not from the LLM.

### Why This Matters for Intelligence

The key insight is that this context is **scoped and precise**:

- **Domain rules** appear only when their domain is active (Context Tree-Shaking)
- **Action suggestions** are computed from the actual data state, not from a static list
- **UI blocks** are server-rendered with a `[SYSTEM]` directive, so the LLM passes them through unchanged instead of trying to recreate them
- **Per-field annotations** tell the LLM exactly which parameters to send, eliminating parameter guessing
- **Embedded Presenter blocks** compose relational context (invoice rules + client rules) into a single response

None of this lives in the system prompt. It all travels **just-in-time** with the data, and only when relevant. The result is that the LLM operates with precise, task-specific context instead of reasoning over a generic, bloated instruction set.

---

## How These Mechanisms Compound

These mechanisms are designed to reinforce each other:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       The Compounding Effect                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Action Consolidation    â†’ significantly fewer tokens in tool schemas   â”‚
â”‚  + TOON Encoding         â†’ ~30-50% fewer tokens in descriptions        â”‚
â”‚  + Cognitive Guardrails  â†’ bounded response tokens on large datasets    â”‚
â”‚  + JIT Context           â†’ no wasted tokens on irrelevant rules         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  = Fewer INPUT TOKENS per call                                          â”‚
â”‚                                                                         â”‚
â”‚  Zod .strict()            â†’ fewer hallucinated-parameter retries         â”‚
â”‚  + Self-Healing Errors   â†’ fewer correction attempts needed             â”‚
â”‚  + Agentic Affordances   â†’ fewer wrong-tool selections                  â”‚
â”‚  + State Sync            â†’ fewer stale-data re-reads                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  = Fewer TOTAL REQUESTS                                                 â”‚
â”‚                                                                         â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  COMBINED GOAL       â†’   â”‚  Lower total cost    â”‚                       â”‚
â”‚                          â”‚  Faster UX           â”‚                       â”‚
â”‚                          â”‚  Less hallucination  â”‚                       â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### An Illustrative Scenario

Consider the AI agent from the Before & After section â€” 50 operations across users, projects, billing, tasks, and reports:

| Metric | Raw MCP Server | With mcp-fusion |
|---|---|---|
| Tools in `tools/list` | 50 | 5 (grouped) |
| Prompt schema tokens | ~10,000 | ~1,670 |
| System prompt domain rules | ~2,000 tokens (global) | 0 (JIT per response) |
| Total prompt tax per turn | ~12,000 | ~1,670 |
| Description format | Plain text | TOON (~40-50% fewer tokens) |
| Response to `tasks.list` (10K rows) | ~5,000,000 tokens | ~25,000 tokens (`.agentLimit()`) |
| Parameter hallucination handling | None â€” leaks to handler | `.strict()` rejects with actionable error |
| Error guidance | Generic message | Directed correction prompt |
| Stale-data awareness | None | `[Cache-Control]` directives |

The exact savings depend on the workload, model, and use case. Our design goal is to make the difference meaningful at scale.

---

## Token Budget Awareness

We believe developers should be able to measure their token footprint before deployment. mcp-fusion includes a preview tool for this:

```typescript
// From: src/framework/builder/GroupedToolBuilder.ts
const projects = defineTool<AppContext>('projects', { ... });
console.log(projects.previewPrompt());

// Output:
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚  MCP Tool Preview: projects                                â”‚
// â”œâ”€â”€â”€ Description â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
// â”‚  Manage workspace projects. Actions: list, create, ...     â”‚
// â”œâ”€â”€â”€ Input Schema â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
// â”‚  { "type": "object", ...  }                                â”‚
// â”œâ”€â”€â”€ Token Estimate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
// â”‚  ~342 tokens (1,368 chars)                                 â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This lets you see exactly what the LLM receives and estimate the token cost â€” before running a single request.

---

## Summary

Every mechanism in mcp-fusion is guided by one equation:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚      â†“ Tokens per call  Ã—  â†“ Calls per task  =  â†“â†“ Total Cost          â”‚
â”‚                                                                         â”‚
â”‚      â†“ Noise in context  +  â†‘ Signal quality  =  â†“â†“ Hallucination      â”‚
â”‚                                                                         â”‚
â”‚      â†“ Retries  +  â†“ Latency per call  =  â†‘â†‘ Response Speed            â”‚
â”‚                                                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•     â”‚
â”‚                                                                         â”‚
â”‚      Fewer tokens. Fewer requests. Faster answers. Lower bills.         â”‚
â”‚      This is the goal we're building toward.                            â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

We're not claiming perfection â€” we're sharing the design principles and mechanisms that guide our work. The code is open, the results are measurable, and we welcome scrutiny.

---

## Next Steps

- [The MVA Manifesto â†’](/mva-pattern) â€” The architectural pattern behind these mechanisms
- [Performance â†’](/performance) â€” Runtime optimizations and benchmarks
- [Building Tools â†’](/building-tools) â€” Implement with `defineTool()` and `createTool()`
- [Presenter â†’](/presenter) â€” Configure guardrails, rules, and affordances
