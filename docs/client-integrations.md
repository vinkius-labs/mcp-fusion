---
description: How to use MCP Fusion backend servers with Vercel AI SDK, LangChain, and LlamaIndex to build robust AI applications.
---

# Client SDK Integrations

If you are building an AI application, you will eventually need to connect your frontend or orchestration layer (like **Vercel AI SDK**, **LangChain**, or **LlamaIndex**) to your backend systems.

While those frameworks are excellent for chatting with LLMs and managing client UI state, they are not designed to be enterprise backend servers. They lack middleware pipelines, context derivation, database guardrails, and tenant isolation. 

This is where **MCP Fusion** becomes the perfect complementary backend. 

## Does MCP Fusion work with Vercel AI SDK and LangChain?

**Yes. MCP Fusion is the perfect backend Server.** If you are using LangChain, LlamaIndex, or Vercel AI SDK in your client application, simply connect them to your `mcp-fusion` backend via stdio or standard HTTP transports. They will automatically consume and execute your **Consolidated MVA Actions** flawlessly.

By using MCP Fusion as the backend tool-provider, you get the best of both worlds:
1. **Frontend:** The rich UI streams, standard prompt templates, and chat histories of Vercel AI SDK / LangChain.
2. **Backend:** The hardcore, enterprise-grade stability of MCP Fusion's Zero-Trust architecture, Zod security stripping, and Deterministic AI Tool Execution.

---

## Vercel AI SDK Integration

When using the Vercel AI SDK in a Next.js application, you can connect your MCP Fusion server to the `useChat` or `streamText` APIs.

The Vercel AI SDK will natively read the tool schemas generated by your MCP Fusion Presenters, and expose them to the LLM. 

### Why use MCP Fusion instead of defining tools directly in Vercel AI SDK?
Defining tools directly in Vercel AI SDK `tool()` blocks works for simple scripts, but fails in production:
- It mixes UI routing with database logic.
- It lacks action consolidation, flooding the system prompt with dozens of tools.
- It has no built-in protection against **Context DDoS** when a query returns too much data.

MCP Fusion solves this by keeping the Vercel AI SDK focused on the UI, while the MCP server handles the Heavy, State-Aware, Guardrailed backend execution.

---

## LangChain Integration

LangChain provides extensive orchestration capabilities. By connecting it to an MCP Fusion server via the `@modelcontextprotocol/sdk` client, your LangChain agents gain immediate access to your entire backend.

### Avoiding the LangChain Tool Hell
A common problem in LangChain is giving an agent 50 different tools (e.g., `list_users`, `create_user`, `delete_user`) which confuses the planner and wastes thousands of tokens.

By using MCP Fusion as the provider, your LangChain agent will see **Consolidated MVA Actions**. Instead of 50 tools, it sees a single `system_orchestrator` tool with a deterministic discriminator pattern. This dramatically improves LangChain agent accuracy and reduces token costs.

---

## LlamaIndex Integration

Similar to LangChain, LlamaIndex agents (like the ReAct agent) can connect to an MCP Fusion server to execute external actions. 

LlamaIndex excels at RAG (Retrieval-Augmented Generation), but struggles when agents need to perform deterministic CRUD mutations safely. By offloading mutations to an MCP Fusion backend, you guarantee that every state change passes through strictly typed middleware and Presenter logic, preventing LLM-driven data corruption.
